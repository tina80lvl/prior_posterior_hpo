\documentclass[times,specification,annotation]{itmo-student-thesis}

%% Опции пакета:
%% - specification - если есть, генерируется задание, иначе не генерируется
%% - annotation - если есть, генерируется аннотация, иначе не генерируется
%% - times - делает все шрифтом Times New Roman, собирается с помощью xelatex
%% - pscyr - делает все шрифтом Times New Roman, требует пакета pscyr.

%% Делает запятую в формулах более интеллектуальной, например:
%% $1,5x$ будет читаться как полтора икса, а не один запятая пять иксов.
%% Однако если написать $1, 5x$, то все будет как прежде.
\usepackage{icomma}

%% Один из пакетов, позволяющий делать таблицы на всю ширину текста.
\usepackage{tabularx}

\usepackage{longtable}

%% Данные пакеты необязательны к использованию в бакалаврских/магистерских
%% Они нужны для иллюстративных целей
%% Начало
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{filecontents}
\usepackage[export]{adjustbox}
\begin{filecontents}{bachelor-thesis.bib}
	@incollection{feurer-automlbook19a,
		author    = {Feurer, Matthias and Hutter, Frank},
		title     = {Hyperparameter Optimization},
		year      = {2019},
		month     = may,
		editor    = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
		booktitle = {AutoML: Methods, Sytems, Challenges},
		publisher = {Springer},
		chapter   = {1},
		pages     = {3--33},
	}
	@misc{lindauer2017warmstarting,
		title={Warmstarting of Model-based Algorithm Configuration},
		author={Marius Lindauer and Frank Hutter},
		year={2017},
		eprint={1709.04636},
		archivePrefix={arXiv},
		primaryClass={cs.AI}
	}
	@techreport{HutHooLey10-TR,
		author = {Hutter, F. and Hoos, H.~H. and Leyton-Brown, K.},
		title = {Sequential Model-Based Optimization for General Algorithm Configuration (extended version)},
		institution = {University of British Columbia, Department of Computer Science},
		year = {2010},
		number = {TR-2010-10},
		note = {Available online: {http://www.cs.ubc.ca/\~{}hutter/papers/10-TR-SMAC.pdf}}
	}
	@incollection{NIPS2015_5872,
		title = {Efficient and Robust Automated Machine Learning},
		author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
		booktitle = {Advances in Neural Information Processing Systems 28},
		editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
		pages = {2962--2970},
		year = {2015},
		publisher = {Curran Associates, Inc.},
		url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf}
	}
	@InProceedings{falkner-icml-18,
		title =        {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},
		author =       {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
		booktitle =    {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},
		pages =        {1436--1445},
		year =         {2018},
		month =        jul,
	}
	@incollection{NIPS2011_4443,
		title = {Algorithms for Hyper-Parameter Optimization},
		author = {James S. Bergstra and Bardenet, R\'{e}mi and Bengio, Yoshua and Bal\'{a}zs K\'{e}gl},
		booktitle = {Advances in Neural Information Processing Systems 24},
		editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
		pages = {2546--2554},
		year = {2011},
		publisher = {Curran Associates, Inc.},
		url = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf}
	}

\end{filecontents}

%% Указываем файл с библиографией.
\addbibresource{bachelor-thesis.bib}

\begin{document}
	
	\studygroup{M3435}
	\title{Алгоритмы настройки гиперпараметров на основе объединения априорных и апостериорных знаний о задаче}
	\author{Смирнова Валентина Сергеевна}{Смирнова В.С.}
	\supervisor{Фильченков Андрей Александрович}{Фильченков А.А.}{к.ф.-м.н}{доцент факультета информационных технологий и программирования, Университет ИТМО}
	\publishyear{2020}
	%% Дата выдачи задания. Можно не указывать, тогда надо будет заполнить от руки.
	%% Срок сдачи студентом работы. Можно не указывать, тогда надо будет заполнить от руки.
	%% Дата защиты. Можно не указывать, тогда надо будет заполнить от руки.
	\defencedate{18}{июня}{2019}
	
	\secretary{Павлова О.Н.}
	
	
	%% Задание
	%%% Техническое задание и исходные данные к работе
	\technicalspec{Требуется разработать алгоритм настройки гиперпараметров на основе объединения априорных и апостериорных знаний о задаче}
	
	%%% Содержание выпускной квалификационной работы (перечень подлежащих разработке вопросов)
	\plannedcontents{В работе должна быть показана эффективность разработанного решения по сравнению с сущесствующими}
	
	%%% Исходные материалы и пособия 
	\plannedsources{\begin{enumerate}
			\item Hutter, F., Hoos, H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. // LION’11. — 2011
			\item Efficient and robust automated machine learning / M. Feurer  // Advances in neural information processing systems. — 2015.
			\item Leite R., Brazdil P. Active Testing Strategy to Predict the Best Classification Algorithm via Sampling and Metalearning. // ECAI. — 2010. 
	\end{enumerate}}

	%% Аннотация
	%%% Цель исследования
	\researchaim{Разработать алгоритм настройки гиперпараметров на основе объединения априорных и апостериорных знаний о задаче}
	
	%%% Задачи, решаемые в ВКР
	\researchtargets{\begin{enumerate}
			\item изучить существующие решения поставленной задачи;
			\item предложить и реализовать новый алгоритм;
			\item провести эксперименты, показывающие эффективность решения.
	\end{enumerate}}

	%%% Использование информационных ресурсов Internet
	%%%\internetsources{1}
	
	%%% Использование современных пакетов компьютерных программ и технологий
	\addadvancedsoftware{Пакет \texttt{numpy} }{Не предусмотрено}
	\addadvancedsoftware{Пакет \texttt{pandas} }{Не предусмотрено}
	\addadvancedsoftware{Пакет \texttt{robo} }{Не предусмотрено}
	\addadvancedsoftware{Пакет \texttt{matplotlib} }{Не предусмотрено}
	\addadvancedsoftware{Пакет \texttt{george} }{Не предусмотрено}	
	
	%%% Краткая характеристика полученных результатов 
	\researchsummary{ Был предложен и реализован эффективный алгоритм, решающий поставленную задачу. }
	
	%%% Гранты, полученные при выполнении работы 
	\researchfunding{Отсутствуют}
	
	%%% Наличие публикаций и выступлений на конференциях по теме выпускной работы
	\researchpublications{\begin{enumerate}
			\item IX Конгресс Молодых Учёных 	
	\end{enumerate}}
	
	
	%% Эта команда генерирует титульный лист и аннотацию.
	\maketitle{Бакалавр}
	
	%% Оглавление
	\tableofcontents
	
	%% Макрос для введения. Совместим со старым стилевиком.
	\startprefacepage
	Задача классификации -- постросить алгоритм (классификатор), который по набору признаков вернул бы метку класса или вектор оценок принадлежности (апостериорных вероятностей) к каждому из классов. Оновная её цель -- максимально точно определить метку класса для заданного объекта. Задача широко применяется во многих областях:
	\begin{itemize}
		\item Медицинская диагностика: по набору медицинских характеристик требуется поставить диагноз
		\item Геологоразведка: по данным зондирования почв определить наличие полезных ископаемых
		\item Оптическое распознавание текстов: по отсканированному изображению текста определить цепочку символов, его формирующих
		\item Кредитный скоринг: по анкете заемщика принять решение о выдаче/отказе кредита
		\item Синтез химических соединений: по параметрам химических элементов спрогнозировать свойства получаемого соединения
	\end{itemize}
	
	Найти и обучить эффективный алгоритм классификации -- трудоёмкая задача, которая включает в себя выбор самого классификатора, сбор и разметку данных (в случае обучения с учителем), и непосредственно настройку гиперпараметров классификатора. Так как гиперпараметры задаются до начала обучения и не изменяются в его ходе, а при этом могут существенно влиять на результат обучения, то повляется задача оптимизации гиперпараметров. \par
	

	\textbf{Оптимизация гиперпараметров} — задача машинного обучения по выбору набора оптимальных гиперпараметров для обучающего алгоритма. Одни и те же виды моделей машинного обучения могут требовать различные предположения, веса или скорости обучения для различных видов данных. Эти параметры называются гиперпараметрами и их следует настраивать так, чтобы модель могла оптимально решить задачу обучения. Для этого находится кортеж гиперпараметров, который даёт оптимальную модель, оптимизирующую заданную функцию потерь на заданных независимых данных. Такая задача несёт название AutoML \cite{feurer-automlbook19a}, основная её задача -- сделать процесс машинного обучения доступным не только для экспертов в области ML, но и для любого пользователя. 
	
	Интерес к задаче AutoMl растет, проводятся конференции и соревнования по её решению. В частности, каждые два года проходит соревнование AutoML Challenge. В августе 2019 года происходило мероприятие, посвящённое этой теме -- The Third International Workshop on Automation in Machine Learning. Согласно выпуску Forbes за декабрь 2018 года, это один из пяти трендов в развитии машинного обучения в 2019 году. Тема AutoML появляется все чаще и чаще в дискуссиях и публикациях. Решения этой задачи уже используются в автономных машинах, предсказании цен и многих других областях.
	
	Существующие решения \cite{lindauer2017warmstarting, HutHooLey10-TR, NIPS2015_5872, falkner-icml-18} для задачи оптимизации гиперпараметров основываются на случайной расстановке гиперпараметров и дальнейшей их настройке. В данной работе будет предложен подход, основанный не на случайной расстановке первичных гиперпараметров, а на особом подходе, основанном на результатах обучения смежных задач.\par

	
	%% Начало содержательной части.
	%% Так помечается начало обзора.
	\chapter{Обзор существующих решений}
	Более подробно рассмотрим задачу классификации, алгоритмы ее решения и способы оценки качества полученной классификации. Также опишем понятие модели алгоритма классификации и основные методы настройки её гиперпараметров, применимые ко многим алгоритмам машинного обучения.
	Заметим, что на сегодняшний момент не предложено способов не случайной расстановки гиперпараметров и дальнейшей их настройки, на основе результатов обучения на схожих задачах.
	\startrelatedwork
	
	\section{Определения и ключевые понятия}
	Для начала введем определения и ключевые понятия, которые будут использоваться в дальнейшей работе.
	\begin{itemize}
		\item классификатор -- параметризованный алгоритм, решающий задачу классификации
		\item конфигурация -- фиксированный набор параметров классификатора
		\item алгоритм -- пара из классификатора и его конфигурации 
		\item решаемая задача -- алгоритм с настроенными гиперпараметрами на конкретном датасете
		\item решённая задача -- алгоритм с оптимизированными гиперпараметрами на конкретном датасете
		\item соседняя задача -- ближайшая задача к решаемой
		\item текущее решение (текущая задача) -- решаемая задача, для которой могут использоваться сведения из решённых (соседних) задач
	\end{itemize}
	\section{Обзор классификаторов}
	В данной части рассмотрим популярные модели для решения задачи классификации. Вспомним, что цель задачи классификации -- наиболее точно определять метку класса по заданному объекту. \par
	Итак, наиболее используемые на сегодняшний момент модели классификаторов:
	
	%% TODO добавить подпункты про категориальные признаки, количество гиперпараметров и тд
	\paragraph{Линейная регрессия} Можно представить в виде уравнения, которое описывает прямую, наиболее точно показывающую взаимосвязь между входными переменными X и выходными переменными Y
	\paragraph{Логистическая регрессия} По аналогии с линейной регрессией требуется найти коэффициенты для входных данны, но уже с помощью нелинейной или логистической функции.
	\paragraph{Линейный дискриминантный анализ (LDA)} Состоит из статистических свойств данных, рассчитанных для каждого класса
	\begin{itemize}
		\item Среднее значение для каждого класса
		\item Дисперсия, рассчитанная по всем классам
	\end{itemize}
	\paragraph{Деревья принятия решений} Представима в виде бинарного дерева, где каждый узел представляет собой входную переменную и точку разделения для этой переменной (при условии, что переменная — число)
	\paragraph{Наивный Байесовский классификатор} Состоит из двух типов вероятностей, которые рассчитываются с помощью тренировочных данных:
	\begin{enumerate} 
		\item Вероятность каждого класса
		\item Условная вероятность для каждого класса при каждом значении x
	\end{enumerate}
	\paragraph{K-ближайших соседей (KNN)} Предсказание метки класса делается на основе меток k ближайших соседей
	\paragraph{Метод опорных векторов (SVM)} Суть метода заключается в построении гиперплоскости, разделяющей классы
	\paragraph{Бэггинг и случайный лес (RandomForest)} Один из наиболее эффективных алгоритмов классификации, берётся множество подвыборок из данных, считается среднее значение для каждой, а затем усредняются результаты для получения лучшей оценки действительного среднего значения
	\paragraph{Бустинг и AdaBoost}: принадлежит семейству ансамблевых алгоритмов, суть которых заключается в создании сильного классификатора на основе нескольких слабых
	\paragraph{Многослойный персептрон (Multilayered perceptron)} Класс искусственных нейронных сетей прямого распространения, состоящих как минимум из трех слоёв: входного, скрытого и выходного 

	В работе мы заострим внимание на модели случайного леса как самой эффективной на сегодняшний момент и модели многослойного парцептрона, так как он обладает наибольшим количеством гиперпараметров и также является достаточно эффективным.
	
	\section{Меры оценки качества классификации}
	Кроме выбора алгоритма классификации и его гиперпараметров, обучить одель, нужно ещё каким-то образом оценить качество работы обученного алгоритма. Для этого датасет делится на 2 части: \textit{train} (на которой модель обучается) и \textit{test} или \textit{validate} (на которой оценивается качество классификации). В данной части мы рассмотрим существующие меры оценки качества классификации и выделим сущесственные для нашей задачи. Для этого сначала вспомним базовые понятия:
	\begin{itemize}
		\item Верно-положительными (TP) называются объекты, которые были классифицированы как положительные и действительно являются таковыми
		\item Верно-отрицательными (TN) называются объекты, которые были классифицированы как отрицательные и действительно таковые 
		\item Ложно-положительными (FP) называются объекты, которые были классифицированы как положительные, но фактически отрицательные
		\item Ложно-отрицательными (FN) называются объекты, которые были классифицированы как отрицательные, но фактически положительные
	\end{itemize}

	\begin{equation}
	Accuracy = \frac{TP+TN}{TP+TN+FP+FN} 
	\label{eq:accuracy}
	\end{equation}
	
	\begin{equation} 
	Precision = \frac{TP}{TP+FP} 
	\label{eq:precision}
	\end{equation}
	
	\begin{equation}
	Recall =  \frac{TP}{TP+FN}
	\label{eq:recall}
	\end{equation}
	
	Одна из наиболее простых и популярных мер оценки качества -- \textit{F-мера} или \textit{F-score}. Считается она следующим образом:
	
	\begin{equation}
 	\mathit{F-score} =  2 * \frac{precision*recall}{precision+recall} 
 	\label{eq:fscore}
	\end{equation}
		
	Приемущество данной меры в том, что её достаточно просто считать и результат отлично подходит для целевой функции оптимизации, о которой мы поговорим в следующей части.\par
	
	Также существует \textit{кривая ошибки} или \textit{ROC-curve (Receiver Operating Characteristic)}. Суть данной меры состоит в том, что считается площать кривой под графиком уровня верно-положительных предсказанных экземпляров от уровня ложно-положительных. Не будем заострять на ней внимание, так как к нашей задаче она не подходит из-за трудоёмкости расчёта. 
	
	
	\section{Обзор подходов к оптимизации гиперпараметров}
	Существует несколько подходов к задаче оптимизации гиперпараметров. В данной части рассмотрим наиболее популярные из них, оценим приемущества и недостатки.
	\paragraph{Поиск по решётке} По сути, данный алгоритм делает полный перебор всех возможных моделей и конфигураций.
		\subparagraph{Доступные реализации}
		\begin{itemize}
			\item LIBSVM
			\item scikit-learn
			\item Talos
		\end{itemize}
		\subparagraph{Достоинства} Гарантированна будет найдена наилучшая конфигурация.
		\subparagraph{Недостатки} Слишком большие затраты на обучение.
	\paragraph{Случайный поиск} Отличается от поиска по решётке тем, что идёт не полный перебор всех конфигурации, а случайная их выборка.
		\subparagraph{Доступные реализации}
		\begin{itemize}
			\item hyperopt
			\item scikit-learn
			\item H2O AutoML
			\item Talos
		\end{itemize}
		\subparagraph{Достоинства} Меньшие затраты на обучение. Существует вероятность нахождения наилучшей конфигурации за наименьшее время.
		\subparagraph{Недостатки} Неопределённое количество времени на поиск наилучшей конфигурации.
	\paragraph{Байесовская оптимизация} Метод, основанный на обращении к функции <<чёрного ящика>> с шумом. Для задачи оптимизации гиперпараметров строит стохастическую модель из отображения из конфигурации в целевую функцию, применённую на валидационном наборе данных.
		\subparagraph{Доступные реализации}
		\begin{itemize}
			\item Spearmint
			\item Bayesopt
			\item MOE
			\item Auto-WEKA
			\item Auto-sklearn
			\item mlrMBO
			\item tuneRanger
			\item BOCS
			\item SMAC
		\end{itemize}
		\subparagraph{Достоинства} Небольшие затраты на обучение, количество итераций задаётся вручную, адаптируется под значимость каждого гиперпараметра для конкретной задачи.
		\subparagraph{Недостатки} Сложность реализации и использования.
	\paragraph{Оптимизация на основе градиентов}  Для определённых алгоритмов обучения вычисляется градиент гиперпараметров и оптимизируется с помощью градиентного спуска.
		\subparagraph{Доступные реализации}
		\begin{itemize}
			\item hypergrad
		\end{itemize}
		\subparagraph{Достоинства} Небольшие затраты на обучение, неплохой результат.
		\subparagraph{Недостатки} Сложность понимания и реализации, небольшое количество доступных реализаций.
	\paragraph{Эволюционная оптимизация} Также, как и Байесовская оптимизация, основывается на обращениях к функции <<чёрного ящика>> с шумом, однако для поиска гиперпараметров для заданного алгоритма использует эволюционные подходы (алгоритмы)\cite{NIPS2011_4443}.
		\subparagraph{Доступные реализации}
		\begin{itemize}
			\item TPOT
			\item devol
			\item deap
		\end{itemize}
		\subparagraph{Достоинства} Небольшие затраты на обучение, неплохой результат.
		\subparagraph{Недостатки} Используется только для статистических алгоритмов. 
		
	Давайте немного подробнее рассмотрим Байесовскую оптимизацию.
	
	\subsection{Байесовская оптимизация}
	Как уже говорилось в предыдущей части, Байесовская оптимизация основывается на обращении к функции <<чёрного ящика>> с шумом. Кроме того, необходимо определить модель, на которой будет основываться Байесовская оптимизация, максимизатор, способ задания первичных гиперпараметров и непосредственно целевую функцию
	
	
	\chapterconclusion
	лалалалал
	
	\chapter{Предложенное решение}
	аоаоа
	\section{Метрика для определения подобия задач}
	оаоа
	\section{Параметры для оптимизации}
	аоао\cite{thornton2013auto,feurer2015efficient,feurer2018practical}
	\subsection{Классификатор}
	аооаоа
	\subsubsection{Обоснование}
	аоаооа
	\subsubsection{Набор гиперпараметров}
	оаоао
	\subsection{Целевая функция}
	ааооаоа
	\subsection{Модель оптимизации}
	аоаоао
	\section{Расширение Байесовской оптимизации}
	аоаоао
	\chapterconclusion
	аоаооа
	
	\chapter{Анализ полученных результатов}
	аооаоа
	\chapterconclusion
	аоаооа
	
	
	%% Макрос для заключения. Совместим со старым стилевиком.
	\startconclusionpage
	В данном разделе размещается заключение. \ref{sec:app:1}
	
	\printmainbibliography
	
	
	%% После этой команды chapter будет генерировать приложения, нумерованные русскими буквами.
	%% \startappendices из старого стилевика будет делать то же самое
	\appendix
	\chapter{Результаты наивной Байесовской оптимизации}\label{sec:app:1}
	\begin{center}
		\begin{longtable}{ |m{5cm}|c|c| } 
			\hline
			\textbf{Имя датасета} & \textbf{Лучшее значение (1 - F-score)} & \textbf{Номер итерации} \\ 
			\hline\hline
			page-blocks & 0.1572831335432724 & 42 \\
			\hline
			robot-failures-lp1 & 0.14102564102564108 & 33 \\
			\hline
			mfeat-fourier & 0.1475854394148598 & 2 \\
			\hline
			jungle-chess-2pcs-raw-endgame-complete & 0.2719523672987666 & 27 \\
			\hline
			heart-switzerland & 0.6858119658119658 & 91 \\
			\hline
			gas-drift-different-concentrations & 0.01891421038359764 & 77 \\
			\hline
			wall-robot-navigation & 0.01529599728237685 & 57 \\
			\hline
			jungle-chess-2pcs-endgame-panther-elephant & 0.0 & 88 \\
			\hline
			leaf & 0.36017355660212813 & 73 \\
			\hline
			PopularKids & 0.06352989828599576 & 40 \\
			\hline
			mfeat-karhunen & 0.014079861405182248 & 20 \\
			\hline
			diggle-table-a2 & 0.9743589743589743 & 84 \\
			\hline
			rmftsa-sleepdata & 0.9278268809884568 & 27 \\
			\hline
			semeion & 0.05376620700232804 & 28 \\
			\hline
			desharnais & 0.2667332667332668 & 86 \\
			\hline
			teachingAssistant & 0.4071969696969696 & 44 \\
			\hline
			collins & 0.8884880528144286 & 38 \\
			\hline
			volcanoes-a3 & 0.7689402697495183 & 61 \\
			\hline
			artificial-characters & 0.3613704595813323 & 30 \\
			\hline
			volcanoes-a4 & 0.7972789115646258 & 51 \\
			\hline
			glass & 0.5231009070294785 & 68 \\
			\hline
			nursery & 0.04052981019439761 & 61 \\
			\hline
			shuttle & 0.0 & 30 \\
			\hline
			segment & 0.02531199490574576 & 62 \\
			\hline
			heart-long-beach & 0.652975912975913 & 91 \\
			\hline
			vertebra-column & 0.20623124372223145 & 80 \\
			\hline
			cnae-9 & 0.034818671429924564 & 26 \\
			\hline
			jannis & 0.9999969017034435 & 10 \\
			\hline
			wine-quality-white & 0.750911817087637 & 40 \\
			\hline
			vehicle & 0.34988621629488503 & 36 \\
			\hline
			ecoli & 0.24709595959595954 & 58 \\
			\hline
			eye-movements & 0.5769399013962551 & 16 \\
			\hline
			seeds & 0.11004901960784308 & 96 \\
			\hline
			car & 0.13424741140312846 & 74 \\
			\hline
			fabert & 0.8754012841091493 & 0 \\
			\hline
			breast-tissue & 0.6674242424242425 & 64 \\
			\hline
			thyroid-allbp & 0.5548263021310648 & 81 \\
			\hline
			gas-drift & 0.011142829995818726 & 81 \\
			\hline
			mfeat-factors & 0.08802270506646293 & 53 \\
			\hline
			volcanoes-d1 & 0.7974875207986689 & 10 \\
			\hline
			har & 0.013956849985395814 & 39 \\
			\hline
			satimage & 0.10895145985989307 & 73 \\
			\hline
			Fashion-MNIST & 0.1732567305672541 & 47 \\
			\hline
			seismic-bumps & 0.03781821523757012 & 34 \\
			\hline
			pokerhand & 0.28102343233242333 & 84 \\
			\hline
			helena & 0.981850107656847 & 98 \\
			\hline
			thyroid-allhyper & 0.5208075903466287 & 14 \\
			\hline
			wine & 0.9846153846153847 & 99 \\
			\hline
			balance-scale & 0.07249605495219524 & 46 \\
			\hline
			microaggregation2 & 0.5627566824728257 & 44 \\
			\hline
			steel-plates-fault & 0.7677201034057892 & 61 \\
			\hline
			tae & 0.3234408602150539 & 22 \\
			\hline
			mfeat-pixel & 0.5312663695353331 & 92 \\
			\hline
			gina-prior2 & 0.08624343753649555 & 81 \\
			\hline
			synthetic-control & 0.0 & 7 \\
			\hline
			cmc & 0.4217043857598791 & 12 \\
			\hline
			energy-efficiency & 0.8637585814858542 & 22 \\
			\hline
			iris & 0.0 & 2 \\
			\hline
			fars & 0.4350632170299382 & 22 \\
			\hline
			abalone & 0.6296727096012757 & 95 \\
			\hline
			prnn-viruses & 0.0 & 62 \\
			\hline
			Indian-pines & 0.5687291627863662 & 4 \\
			\hline
			covertype & 0.4420032958440877 & 5 \\
			\hline
			JapaneseVowels & 0.44703798338069056 & 91 \\
			\hline
			user-knowledge & 0.1050103519668737 & 29 \\
			\hline
			spectrometer & 0.9662337662337662 & 47 \\
			\hline
			hayes-roth & 0.12222222222222212 & 34 \\
			\hline
			robot-failures-lp5 & 0.2971834250941756 & 59 \\
			\hline
			prnn-fglass & 0.3836808236808237 & 27 \\
			\hline
			waveform-5000 & 0.12700439195696356 & 27 \\
			\hline
			zoo & 0.0 & 12 \\
			\hline
			cardiotocography & 0.06492996280208396 & 96 \\
			\hline
			mfeat-morphological & 0.5482421206646882 & 6 \\
			\hline
			volcanoes-a1 & 0.8019336485603352 & 98 \\
			\hline
			tamilnadu-electricity & 0.0755314544908181 & 40 \\
			\hline
			LED-display-domain-7digit & 0.21607382867705938 & 44 \\
			\hline
			
		\end{longtable}
	\end{center}
	
\end{document}